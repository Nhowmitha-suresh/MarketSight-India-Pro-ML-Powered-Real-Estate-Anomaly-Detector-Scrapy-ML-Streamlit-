"""
Setup helper for configuring the Real Estate Monitoring Pipeline.
"""
import os
import sys

def create_env_file():
    """Create a .env file from the template."""
    template = '.env.example'
    target = '.env'
    
    if os.path.exists(target):
        print(f'â„¹ï¸  {target} already exists. Skipping...')
        return
    
    if not os.path.exists(template):
        print(f'âŒ {template} not found.')
        return
    
    with open(template, 'r') as f:
        content = f.read()
    
    with open(target, 'w') as f:
        f.write(content)
    
    print(f'âœ… Created {target} from {template}')
    print('\nğŸ“ IMPORTANT: Edit .env and configure:')
    print('   - DATABASE_URL (PostgreSQL connection string)')
    print('   - TARGET_ZIP (the ZIP code to scrape)')


def print_setup_guide():
    """Print setup instructions."""
    guide = '''
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  REAL ESTATE MONITORING PIPELINE SETUP                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… WHAT WAS TESTED:
   âœ“ Data ingestion & cleaning (Scrapy pipeline simulation)
   âœ“ Price/sqft calculation & normalization
   âœ“ Statistical analysis (mean, std deviation)
   âœ“ Anomaly detection (1.5Ïƒ threshold)
   âœ“ Database persistence
   âœ“ Analysis result storage

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ NEXT STEPS TO DEPLOY:

1ï¸âƒ£  INSTALL DEPENDENCIES
    python -m venv .venv
    .venv\\Scripts\\Activate.ps1  # on Windows PowerShell
    pip install -r requirements.txt

2ï¸âƒ£  CONFIGURE DATABASE
    Option A: Use PostgreSQL (Recommended)
      â€¢ Install PostgreSQL and create a database
      â€¢ Create .env and set DATABASE_URL
      â€¢ Run SQL schema:
        psql -U <user> -d <dbname> -f sql/create_tables.sql
    
    Option B: Use SQLite (For Testing)
      â€¢ Set DATABASE_URL to: sqlite:///./realestate.db
      â€¢ Schema tables are auto-created if not exists

3ï¸âƒ£  CONFIGURE .env FILE
    Copy .env.example to .env and edit:
      DATABASE_URL=postgresql://user:password@localhost/realestate_db
      TARGET_ZIP=<your_target_zip>
      SCRAPE_INTERVAL_HOURS=24

4ï¸âƒ£  CUSTOMIZE SCRAPER
    Edit realestate_scraper/spiders/listings_spider.py:
      â€¢ Update CSS selectors to match your target website
      â€¢ Modify SCRAPY_START_URL in .env
      â€¢ If site uses JavaScript, set USE_PLAYWRIGHT=True

5ï¸âƒ£  RUN THE PIPELINE

    a) ONE-TIME SCRAPE:
       scrapy crawl listings -a zip=<target_zip>

    b) AUTOMATED DAILY SCRAPING:
       python scheduler/run_daily.py
       (keeps running; scrapes + analyzes every 24 hours)

    c) MANUAL ANALYSIS (after scraping):
       python analysis/analyze.py

    d) VIEW DASHBOARD:
       streamlit run app/streamlit_app.py
       (Open http://localhost:8501 in browser)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š DASHBOARD FEATURES:

  â€¢ Executive Overview: Total listings, anomalies, avg $/sqft, avg days on market
  â€¢ Anomaly Finder: Filterable list of under/over-priced properties
  â€¢ Market Trends: Time-series charts of market statistics
  â€¢ Alert Settings: Placeholder for custom alerts (future feature)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš™ï¸  PROJECT STRUCTURE:

  realestate_monitoring/
  â”œâ”€â”€ realestate_scraper/          # Scrapy spider & pipelines
  â”‚   â”œâ”€â”€ spiders/listings_spider.py
  â”‚   â”œâ”€â”€ pipelines.py
  â”‚   â”œâ”€â”€ items.py
  â”‚   â””â”€â”€ settings.py
  â”œâ”€â”€ analysis/
  â”‚   â””â”€â”€ analyze.py               # Statistical analysis & anomaly detection
  â”œâ”€â”€ scheduler/
  â”‚   â””â”€â”€ run_daily.py             # APScheduler for automated runs
  â”œâ”€â”€ app/
  â”‚   â”œâ”€â”€ streamlit_app.py         # High-end dashboard UI
  â”‚   â””â”€â”€ utils.py                 # Helper functions
  â”œâ”€â”€ sql/
  â”‚   â””â”€â”€ create_tables.sql        # PostgreSQL schema
  â”œâ”€â”€ requirements.txt             # Python dependencies
  â”œâ”€â”€ .env.example                 # Template env vars
  â”œâ”€â”€ README.md                    # This file
  â””â”€â”€ test_e2e.py                 # End-to-end test harness

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ QUICK TIPS:

  â€¢ Use test_e2e.py to validate your setup locally before deploying
  â€¢ The pipeline cleans & normalizes raw data automatically
  â€¢ Anomalies are flagged using statistical Z-score (1.5Ïƒ threshold)
  â€¢ Group stats are stored for time-series trending
  â€¢ Streamlit auto-caches queries for 5 minutes (configurable)
  â€¢ The scheduler can run 24/7 for continuous market monitoring

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸  LEGAL & ETHICAL CONSIDERATIONS:

  â€¢ Always check robots.txt and terms of service before scraping
  â€¢ Respect rate limits (current: 1 sec delay, 8 concurrent requests)
  â€¢ Do not scrape sites that explicitly forbid automated access
  â€¢ Use official APIs when available
  â€¢ Consider obtaining explicit permission for commercial use
  â€¢ Ensure GDPR/privacy compliance when storing personal data

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
'''
    print(guide)


if __name__ == '__main__':
    print('\nğŸš€ Setting up Real Estate Monitoring Pipeline...\n')
    create_env_file()
    print()
    print_setup_guide()
